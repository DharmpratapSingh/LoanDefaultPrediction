# Loan Default Prediction Project

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A production-ready machine learning project for predicting loan defaults using borrower attributes. This project has been refactored with modular code, proper logging, error handling, and an inference pipeline for real-world deployment.

## üöÄ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Train all models
python -m src.main --mode train

# Make predictions
python -m src.inference
```

See [USAGE.md](USAGE.md) for detailed instructions.

## üìÅ Project Structure

```
LoanDefaultPrediction/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml              # Centralized configuration
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # Main training pipeline
‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py    # Data preprocessing module
‚îÇ   ‚îú‚îÄ‚îÄ model_training.py        # Model training module
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py            # Model evaluation module
‚îÇ   ‚îú‚îÄ‚îÄ inference.py             # Production inference pipeline
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ logger.py            # Logging utilities
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ LoanDefaultPrediction.ipynb  # Original exploratory notebook
‚îú‚îÄ‚îÄ models/                      # Saved model artifacts
‚îú‚îÄ‚îÄ logs/                        # Training logs
‚îú‚îÄ‚îÄ reports/                     # Evaluation reports & visualizations
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ README.md                    # This file
‚îî‚îÄ‚îÄ USAGE.md                     # Detailed usage guide
```

## ‚ú® Key Features

### Production-Ready Code
- **Modular Architecture**: Clean separation of concerns (preprocessing, training, evaluation, inference)
- **Error Handling**: Comprehensive try-catch blocks with detailed logging
- **Configuration Management**: YAML-based configuration for easy experimentation
- **Logging**: Timestamped logs for debugging and monitoring
- **Type Hints**: Better code readability and IDE support

### Model Persistence
- Save and load trained models
- Persistent preprocessing artifacts (scalers, PCA)
- Easy model versioning

### Inference Pipeline
- Single prediction API
- Batch prediction from CSV
- Risk categorization (Low/Medium/High)
- Probability estimates

### Fixed Issues
- ‚úÖ Resolved deprecated pandas `.fillna(inplace=True)` warnings
- ‚úÖ Proper error handling throughout
- ‚úÖ Modular code instead of monolithic notebook
- ‚úÖ Configuration-driven hyperparameters
- ‚úÖ Production-ready inference API

## üìä Model Performance

| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC |
|-------|----------|-----------|--------|----------|---------|
| **Logistic Regression** | 0.86 | 0.86 | 0.86 | 0.86 | **0.92** |
| **Random Forest** | 0.84 | 0.84 | 0.84 | 0.84 | **0.92** |
| **XGBoost** | 0.81 | 0.82 | 0.81 | 0.81 | 0.88 |
| **Lasso Regression** | 0.86 | 0.87 | 0.87 | 0.86 | 0.91 |
| **Stacking Ensemble** | 0.84 | 0.84 | 0.84 | 0.84 | 0.92 |

## üîß Installation

### Requirements
- Python 3.8+
- pip

### Setup
```bash
# Clone repository
git clone <repository-url>
cd LoanDefaultPrediction

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## üíª Usage Examples

### Training Models
```bash
# Train all models (except neural network)
python -m src.main --mode train

# Train with neural network
python -m src.main --mode train --include-neural-net

# Train linear models only
python -m src.main --mode train_linear
```

### Making Predictions
```python
from src.inference import LoanDefaultPredictor

# Initialize predictor
predictor = LoanDefaultPredictor(model_name="random_forest")

# Single prediction
loan_data = {
    'Age': 30,
    'Income': 65000,
    'Home': 'MORTGAGE',
    'Emp_length': 5.0,
    'Intent': 'EDUCATION',
    'Amount': 10000,
    'Rate': 10.5,
    'Status': 0,
    'Percent_income': 0.15,
    'Cred_length': 5
}

result = predictor.predict_single(loan_data)
print(result)
# {'default': False, 'default_probability': 0.23, 'risk_level': 'Low Risk'}

# Batch predictions
predictions = predictor.predict_batch("loans.csv", "predictions.csv")
```

## üìà What's New in Refactored Version

### Code Quality Improvements
- Modular Python scripts instead of single notebook
- Comprehensive error handling and logging
- Type hints for better code documentation
- Configuration-driven design

### Production Features
- Model persistence (save/load)
- Inference API for real-time predictions
- Batch prediction pipeline
- Risk categorization

### Best Practices
- Virtual environment support
- `.gitignore` for clean repository
- Proper package structure
- Deprecation warnings fixed

## üìñ Documentation

- **[USAGE.md](USAGE.md)** - Comprehensive usage guide
- **[config/config.yaml](config/config.yaml)** - Configuration reference
- **notebooks/** - Original exploratory analysis

---

# Introduction

The project focuses on predicting whether a loan will default based on borrower attributes. This classification problem helps financial institutions assess risk and make informed decisions regarding loan approvals.

# Problem Statement

Loan default prediction is crucial for minimizing financial risks. The aim is to build a machine learning pipeline that:
	‚Ä¢	Predicts loan defaults with high precision and recall.
	‚Ä¢	Handles class imbalance effectively.
	‚Ä¢	Provides interpretability for decision-making.

Objectives
	1.	Preprocess the dataset to handle missing values, outliers, and categorical variables.
	2.	Address class imbalance using SMOTE.
	3.	Build and evaluate multiple machine learning models:
  	‚Ä¢	Logistic Regression
  	‚Ä¢	Lasso Regression
  	‚Ä¢	Random Forest
  	‚Ä¢	XGBoost
  	‚Ä¢	Neural Networks
  	‚Ä¢	SVM
	4.	Optimize the best models using hyperparameter tuning.
	5.	Compare model performance based on evaluation metrics.
	6.	Provide actionable insights through model interpretability.

# Dataset

Source: [Loan Prediction Dataset](https://www.kaggle.com/datasets/ganjerlawrence/loan-risk-prediction-dataset/data)

The dataset contains information on borrowers and their loans, with attributes such as:
	‚Ä¢	Numerical Columns: Age, Income, Loan Amount, etc.
	‚Ä¢	Categorical Columns: Home Ownership, Loan Purpose (Intent).
	‚Ä¢	Target Column: Default (Yes/No).

Key Statistics:
	‚Ä¢	Total Records: 8,145
	‚Ä¢	Class Distribution: Imbalanced (Default = N (majority), Y (minority)).

# Preprocessing Steps

1. Handling Missing Values
	‚Ä¢	Imputed Emp_length with the mode.
	‚Ä¢	Imputed Rate with the median.

2. Encoding Categorical Variables
	‚Ä¢	Applied one-hot encoding to Home and Intent columns:
	‚Ä¢	Dropped one category for linear models to avoid multicollinearity.

3. Addressing Class Imbalance
	‚Ä¢	Used SMOTE to oversample the minority class (Default = Y) to balance the dataset.

4. Scaling
	‚Ä¢	Applied Min-Max Scaling to numerical features to normalize their range.

5. Dimensionality Reduction
	‚Ä¢	Applied PCA to reduce dimensionality and retain 95% variance.

# Modeling and Evaluation

Trained Models:
	1.	Logistic Regression
	2.	Lasso Regression
	3.	Random Forest
	4.	XGBoost
	5.	Neural Networks
	6.	SVM

Evaluation Metrics:
	‚Ä¢	Accuracy
	‚Ä¢	Precision
	‚Ä¢	Recall
	‚Ä¢	F1-Score
	‚Ä¢	ROC-AUC

Key Findings:
	‚Ä¢	Linear Models:
	‚Ä¢	Performed better with dataset_linear (avoiding multicollinearity).
	‚Ä¢	Logistic Regression achieved ROC-AUC = 0.92.
	‚Ä¢	Non-Linear Models:
	‚Ä¢	Random Forest and XGBoost outperformed other models.
	‚Ä¢	Random Forest: ROC-AUC = 0.92.
	‚Ä¢	XGBoost: ROC-AUC = 0.88.
	‚Ä¢	Neural Networks:
	‚Ä¢	Improved after tuning, achieving moderate performance.
	‚Ä¢	SVM:
	‚Ä¢	Underperformed compared to other models.

Hyperparameter Tuning

Random Forest:
	‚Ä¢	Used GridSearchCV to tune n_estimators, max_depth, and other parameters.
	‚Ä¢	Best ROC-AUC: 0.92.

XGBoost:
	‚Ä¢	Used RandomizedSearchCV to tune learning_rate, max_depth, subsample, and colsample_bytree.
	‚Ä¢	Best ROC-AUC: 0.88.

Neural Networks:
	‚Ä¢	Added dropout layers and batch normalization.
	‚Ä¢	Optimized learning rate, batch size, and epochs.

# Final Results and Insights

Model	              Accuracy	Precision	Recall	F1-Score	ROC-AUC
Logistic Regression	0.86	    0.86	    0.86	  0.86	    0.92
Lasso Regression	  0.86	    0.87	    0.87	  0.86	    0.91
Random Forest	      0.84	    0.84	    0.84	  0.84	    0.92
XGBoost	            0.81	    0.82	    0.81	  0.81	    0.88
Neural Networks	    0.60	    0.60	    0.60	  0.60	    -
SVM	                0.57	    0.57	    0.57	  0.56	    0.60

Key Insights:
	1.	Random Forest and XGBoost are the best-performing models.
	2.	Logistic Regression performed well with dataset_linear, achieving similar ROC-AUC as Random Forest.
	3.	Neural Networks require further tuning and more data for competitive performance.

# Conclusion
	‚Ä¢	The Random Forest model is recommended for its balanced performance across all metrics.
	‚Ä¢	XGBoost is a strong alternative, especially for datasets with complex non-linear patterns.
	‚Ä¢	Linear models (Logistic, Lasso) are suitable when interpretability is prioritized.

# Future Work
	1.	Ensemble Models:
	  ‚Ä¢	Combine Random Forest and XGBoost predictions for potential performance gains.
	2.	Feature Engineering:
	  ‚Ä¢	Explore interaction terms and non-linear transformations.
	3.	Advanced Neural Networks:
	  ‚Ä¢	Use deep learning frameworks for larger datasets with more features.
	4.	Explainability:
	  ‚Ä¢	Implement SHAP or LIME to understand feature importance and model behavior.
